{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f969c229-5a8a-44b6-91a3-bba55968b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,imageio,sys,time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "sys.path.append('..')\n",
    "from models.FactorFields import FactorFields \n",
    "\n",
    "from utils import SimpleSampler\n",
    "from dataLoader import dataset_dict\n",
    "from torch.utils.data import DataLoader\n",
    "imageio.plugins.freeimage.download()\n",
    "\n",
    "# device = 'cuda'\n",
    "# torch.cuda.set_device(0)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9acd258-ab23-489a-93a0-7bc799bbab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(a,b):\n",
    "    if type(a).__module__ == np.__name__:\n",
    "        mse = np.mean((a-b)**2)\n",
    "    else:\n",
    "        mse = torch.mean((a-b)**2).item()\n",
    "    psnr = -10.0 * np.log(mse) / np.log(10.0)\n",
    "    return psnr\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_img(reso, chunk=10240):\n",
    "    y = torch.arange(0, reso[0])\n",
    "    x = torch.arange(0, reso[1])\n",
    "    yy, xx = torch.meshgrid((y, x), indexing='ij')\n",
    "    res = []\n",
    "    \n",
    "    coordiantes = torch.stack((xx,yy),dim=-1).reshape(-1,2) + 0.5 #/(torch.FloatTensor(reso[::-1])-1)*2-1\n",
    "    coordiantes = torch.split(coordiantes,chunk,dim=0)\n",
    "    for coordiante in tqdm(coordiantes):\n",
    "\n",
    "        feats,_ = model.get_coding(coordiante.to(model.device))\n",
    "        y_recon = model.linear_mat(feats)\n",
    "        \n",
    "        res.append(y_recon.cpu())\n",
    "    return torch.cat(res).reshape(reso[0],reso[1],-1)\n",
    "\n",
    "def srgb_to_linear(img):\n",
    "\tlimit = 0.04045\n",
    "\treturn np.where(img > limit, np.power((img + 0.055) / 1.055, 2.4), img / 12.92)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee3a42f2-3314-4cf4-ba64-a015cd84d96f",
   "metadata": {},
   "source": [
    "# Data loader\n",
    "### please install the av package with \"pip install av\" if raise a error \"pyav:  pip install imageio[pyav]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2a3772-7637-4087-8890-d5e15122ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_conf = OmegaConf.load('../configs/defaults.yaml')\n",
    "second_conf = OmegaConf.load('../configs/image.yaml')\n",
    "cfg = OmegaConf.merge(base_conf, second_conf)\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "dataset = dataset_dict[cfg.dataset.dataset_name]\n",
    "tolinear = False if cfg.dataset.datadir.endswith('exr') else True\n",
    "train_dataset = dataset(cfg.dataset, cfg.training.batch_size, split='train', tolinear=tolinear)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "              num_workers=8,\n",
    "              persistent_workers=True,\n",
    "              batch_size=None,\n",
    "              pin_memory=True)\n",
    "\n",
    "cfg.model.out_dim = train_dataset.img.shape[-1]\n",
    "batch_size = cfg.training.batch_size\n",
    "n_iter = cfg.training.n_iters\n",
    "\n",
    "H,W = train_dataset.HW\n",
    "cfg.dataset.aabb = train_dataset.scene_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634c9519-4be6-47b8-b030-616bdc4c6237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> total parameters:  1429312\n",
      "FactorFields(\n",
      "  (coeffs): ParameterList(  (0): Parameter containing: [torch.float32 of size 1x144x63x63])\n",
      "  (basises): ParameterList(\n",
      "      (0): Parameter containing: [torch.float32 of size 1x32x32x32]\n",
      "      (1): Parameter containing: [torch.float32 of size 1x32x51x51]\n",
      "      (2): Parameter containing: [torch.float32 of size 1x32x70x70]\n",
      "      (3): Parameter containing: [torch.float32 of size 1x16x89x89]\n",
      "      (4): Parameter containing: [torch.float32 of size 1x16x108x108]\n",
      "      (5): Parameter containing: [torch.float32 of size 1x16x128x128]\n",
      "  )\n",
      "  (linear_mat): MLPMixer(\n",
      "    (backbone): ModuleList(\n",
      "      (0): Linear(in_features=144, out_features=64, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=3, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "total parameters:  1429312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/opt/homebrew/anaconda3/envs/py310/lib/python3.10/site-packages/torch/nn/functional.py:4244: UserWarning: MPS: grid_sampler_2d op is supported natively starting from macOS 13.1. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/GridSampler.mm:139.)\n",
      "  return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "Iteration 00000: loss_dist = 0.12313133 psnr = 9.096:   0%|          | 0/10000 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::grid_sampler_2d_backward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m*\u001b[39m loss_scale\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 34\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m iteration_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart  \n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::grid_sampler_2d_backward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "model = FactorFields(cfg, device)\n",
    "print(model)\n",
    "print('total parameters: ',model.n_parameters())\n",
    "\n",
    "grad_vars = model.get_optparam_groups(lr_small=cfg.training.lr_small,lr_large=cfg.training.lr_large)\n",
    "optimizer = torch.optim.Adam(grad_vars, betas=(0.9, 0.99))#\n",
    "\n",
    "\n",
    "\n",
    "loss_scale = 1.0\n",
    "lr_factor = 0.1 ** (1 / n_iter)\n",
    "pbar = tqdm(range(n_iter))\n",
    "start = time.time()\n",
    "for (iteration, sample) in zip(pbar,train_loader):\n",
    "    loss_scale *= lr_factor\n",
    "\n",
    "    coordiantes, pixel_rgb = sample['xy'], sample['rgb']\n",
    "    feats,coeff = model.get_coding(coordiantes.to(device))\n",
    "    \n",
    "    y_recon = model.linear_mat(feats)\n",
    "    \n",
    "    loss = torch.mean((y_recon.squeeze()-pixel_rgb.to(device))**2) \n",
    "    \n",
    "    \n",
    "    psnr = -10.0 * np.log(loss.item()) / np.log(10.0)\n",
    "    pbar.set_description(\n",
    "                f'Iteration {iteration:05d}:'\n",
    "                + f' loss_dist = {loss.item():.8f}'\n",
    "                + f' psnr = {psnr:.3f}'\n",
    "            )\n",
    "    \n",
    "    loss = loss * loss_scale\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "iteration_time = time.time()-start  \n",
    "    \n",
    "H,W = train_dataset.HW\n",
    "img = eval_img(train_dataset.HW).clamp(0,1.)\n",
    "print(PSNR(img,train_dataset.image.view(img.shape)),iteration_time)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5ef63-9aeb-4d60-a7f0-28ea47b9811f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
